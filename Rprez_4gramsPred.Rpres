```{r setup, include=FALSE}
opts_chunk$set(cache=TRUE)
```
<style>
.reveal h3 {
    color: #3E658E;
    font-weight; 800;
}
body {
    background: url(https://github.com/vinceforce/coursera-capstone/blob/master/desktop-view-alpha.png?raw=true)
}
.bluegrass {
    color: #3E658E;
}
</style>

4-grams next word prediction
========================================================
author: Vincent Force
date: March 23th, 2018
autosize: true
transition: rotate
css: Rprez_custom.css
width: 1440
height: 900

Executive summary
========================================================
transition: zoom

The purpose is to design an algorithm for predicting next word based on the 3 (at most) preceding words, following the main steps above

- <span class="bluegrass" style="font-weight: bold">Data processing</span> : long computations, using the training data, performed at design time to have <span style="color: orange">optimal performance</span> (good accuracy with few memory and cpu usage) at runtime, 
- <span class="bluegrass" style="font-weight: bold">Evaluation and iteration</span> : refining algorithm and <span style="color: orange">measure accuracy</span> at each step on a fixed testing set
- <span class="bluegrass" style="font-weight: bold">Application</span> : design a shiny application to <span style="color: orange">demonstrate algorithm results</span>

The resulting algorithm has to use <span style="color: orange">less than 1GB RAM</span> on runtime to fit the limit on a shinyapps.io with a free plan, and return predictions in a short enough time to be used <span style="color: orange">interactively</span>.

We have to keep in mind that initial target is a <span style="color: orange">smart-phone</span>!

Data processing
====================================
transition: concave
transition-speed: fast
Many steps are involved in data processing, including statistical computations, as well as specific NLP operations and optimisation operations

- <span class="bluegrass" style="font-weight: bold">Vocabulary</span> : based on 1-grams with frequency greater than 1
- <span class="bluegrass" style="font-weight: bold">Profanity words</span> : this type of words are excluded from predictions, even if they were in the training set
- <span class="bluegrass" style="font-weight: bold">Cleaning</span> : sentences with bad words (symbols, numbers, URLs, hashtags, ...) are excluded from training set
- <span class="bluegrass" style="font-weight: bold">Start, end of sentence</span> : added as special words <span style="color: orange">BEGIN</span> and <span style="color: orange">END</span> to sentences
- <span class="bluegrass" style="font-weight: bold">Unknown words</span> : use a special word <span style="color: orange">UNK</span> for replacing out of vocabulary words 
- <span class="bluegrass" style="font-weight: bold">Computation</span> of Maximum Likelihood, Good-Turing Smoothing, Katz discounting and backoff factors
- <span class="bluegrass" style="font-weight: bold">Pruning</span> : items with lowest frequency are deleted, as well as those with <span style="color: orange">UNK</span> and <span style="color: orange">BEGIN</span> prediction

Prediction(s)
====================================
transition: rotate

We tried to use the <span style="color: orange">stemming</span> (e.g. going, gone, go get stemmed to go) methods provided by quanteda package to leverage the number of possible matches.

Four different backoff prediction algorithms have been implemented, in order to perform a <span style="color: orange">benchmark</span>.

1. <span class="bluegrass" style="font-weight: bold">Stupid backoff</span>, using Maximum Likelihood Estimation as N-grams probability, and a constant as backoff factor
2. Same as above, but with <span class="bluegrass" style="font-weight: bold">stemming</span>
3. <span class="bluegrass" style="font-weight: bold">Katz backoff</span>, using Good-Turing smoothing and different backoff factors for each N-gram
4. Same as above, but with <span class="bluegrass" style="font-weight: bold">stemming</span>


Accuracy
====================================
transition: zoom
left: 40%
<small>
Accuracy is given for different training sizes (% of initial data to train the model)
- <span style="color: orange">small</span> for 0.1% / 3 MB RAM at runtime,
- <span style="color: orange">medium</span> for 1% / 20 MB RAM at runtime,
- <span style="color: orange">big</span> for 10% / 200 MB RAM at runtime. 

and for different accuracy levels
- <span style="color: orange">1</span> for proportion of word to guess in 1st prediction
- <span style="color: orange">3</span> for proportion of word to guess in the 3 1st predictions
- <span style="color: orange">10</span> for proportion of word to guess in the 10 1st predictions

*****
```{r echo=FALSE}
library(knitr)
library(kableExtra)
library(ggplot2)
library(reshape2)
acc_table <- data.frame()
acc_table_small <- read.csv2("Rprez_files/small/accuracy_table.csv", sep = ",", stringsAsFactors = FALSE, header = TRUE)
acc_table_small$Acc <- as.numeric(acc_table_small$Acc)
acc_table_small$Acc_3 <- as.numeric(acc_table_small$Acc_3)
acc_table_small$Acc_10 <- as.numeric(acc_table_small$Acc_10)
acc_table <- rbind(acc_table, data.frame(Size = rep("Small", 4), Algorithm = acc_table_small$Type,
        Acc = acc_table_small$Acc, Acc_3 = acc_table_small$Acc_3, Acc_10 = acc_table_small$Acc_10))
```

```{r echo=FALSE}
acc_table_medium <- read.csv2("Rprez_files/medium/accuracy_table.csv", sep = ",", stringsAsFactors = FALSE)
acc_table_medium$Acc <- as.numeric(acc_table_medium$Acc)
acc_table_medium$Acc_3 <- as.numeric(acc_table_medium$Acc_3)
acc_table_medium$Acc_10 <- as.numeric(acc_table_medium$Acc_10)
acc_table <- rbind(acc_table, data.frame(Size = rep("Medium", 4), Algorithm = acc_table_medium$Type, 
        Acc = acc_table_medium$Acc, Acc_3 = acc_table_medium$Acc_3, Acc_10 = acc_table_medium$Acc_10))
```

```{r echo=FALSE, fig.width = 12, fig.height = 9, fig.align = "center"}
acc_table_big <- read.csv2("Rprez_files/big/accuracy_table.csv", sep = ",", stringsAsFactors = FALSE)
acc_table_big$Acc <- as.numeric(acc_table_big$Acc)
acc_table_big$Acc_3 <- as.numeric(acc_table_big$Acc_3)
acc_table_big$Acc_10 <- as.numeric(acc_table_big$Acc_10)
acc_table <- rbind(acc_table, data.frame(Size = rep("Big", 4), Algorithm = acc_table_big$Type,
        Acc = acc_table_big$Acc, Acc_3 = acc_table_big$Acc_3, Acc_10 = acc_table_big$Acc_10))
acc_tableMelt <- melt(acc_table,id=c("Size","Algorithm"),measure.vars=c("Acc","Acc_3", "Acc_10"))
colnames(acc_tableMelt) <- c("Size", "Algorithm", "Level", "Accuracy")
acc_tableMelt$Level <- as.character(acc_tableMelt$Level)
acc_tableMelt[acc_tableMelt$Level == "Acc", "Level"] <- "1"
acc_tableMelt[acc_tableMelt$Level == "Acc_3", "Level"] <- "3"
acc_tableMelt[acc_tableMelt$Level == "Acc_10", "Level"] <- "10"
acc_tableMelt$Level <- as.numeric(acc_tableMelt$Level)
g <- ggplot(acc_tableMelt, aes(Level, Accuracy, colour = Algorithm)) + geom_line(aes(colour = Algorithm), size = 2) + facet_grid(. ~ Size) + scale_x_continuous(breaks=c(1,3, 10)) + scale_y_continuous(breaks=c(0.15,0.2,0.3,0.4)) + theme(
                    legend.title = element_text(size = rel(2)),
                     legend.text = element_text(size = rel(2)),
                    legend.key.size = unit(2, "lines"),
                     axis.title.x = element_text(size = rel(2)),
                     axis.title.y = element_text(size = rel(2)),
                     strip.text.x = element_text(size = rel(2)),
                     axis.text = element_text(size = rel(2)),
                     )
g
```
</small>


Shiny Application
====================================
transition: linear
transition-speed: slow
<img src="Rprez_files/desktop-view.png" height=360 align = "left" hspace = 20 />
&nbsp;&nbsp;&nbsp;&nbsp;A responsive application has been designed for<br />&nbsp;&nbsp;&nbsp;&nbsp;you to test the algorithms.<br /><br />
&nbsp;&nbsp;&nbsp;&nbsp;It provides an interactive next word prediction<br />&nbsp;&nbsp;&nbsp;&nbsp;interface and an accuracy measurement tool on<br />&nbsp;&nbsp;&nbsp;&nbsp;a text given by user<br /><br />
&nbsp;&nbsp;&nbsp;&nbsp;<a href="https://vinceforce.shinyapps.io/Ngrams-predictor" target="_blank" style="text-decoration: underline; color: orange">Click to go to shiny application</a><br /><br />

<span class = "bluegrass" style = "font-weight: bold">Interactive next word prediction interface</span>
<br />Type an input phrase and hit space bar, for the interface to provide:
- <span class = "bluegrass">Suggestions</span> for next word, with buttons to add one suggestion to typed text
- <span class = "bluegrass">3D plots</span> to show the discounted probabilities for n-grams implied in prediction, by level
- <span class = "bluegrass">Word clouds</span> of predictions, all levels together
