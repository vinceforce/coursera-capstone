<html>

<head>
<style type="text/css">
.knitr .inline {
  background-color: #f7f7f7;
  border:solid 1px #B0B0B0;
}
.error {
	font-weight: bold;
	color: #FF0000;
}
.warning {
	font-weight: bold;
}
.message {
	font-style: italic;
}
.source, .output, .warning, .error, .message {
	padding: 0 1em;
  border:solid 1px #F7F7F7;
}
.source {
  background-color: #f5f5f5;
}
.rimage .left {
  text-align: left;
}
.rimage .right {
  text-align: right;
}
.rimage .center {
  text-align: center;
}
.hl.num {
  color: #AF0F91;
}
.hl.str {
  color: #317ECC;
}
.hl.com {
  color: #AD95AF;
  font-style: italic;
}
.hl.opt {
  color: #000000;
}
.hl.std {
  color: #585858;
}
.hl.kwa {
  color: #295F94;
  font-weight: bold;
}
.hl.kwb {
  color: #B05A65;
}
.hl.kwc {
  color: #55aa55;
}
.hl.kwd {
  color: #BC5A65;
  font-weight: bold;
}
</style>
<title>Improvements</title>
<link rel="stylesheet" type="text/css" href="custom.css">
</head>

<body>
<h2>Overall</h2>
<p>We can think of several improvements for going further with this study, including perfecting the methods used, and using other methods.</p>
<h2>Bigger training data</h2>
<p>Provided we have a long time to perform calculations (numerous intermediary steps necessary to achieve them), a bigger training set could be used, with a more radical pruning at the end (frequency < 10, for example).</p>
<h2>Higher level N-grams</h2>
<p>We could compute higher level N-grams (5-grams, 6-grams) to improve accuracy, despite it would lead to longest computations in training phase and higher memory usage on runtime.</p>
<h2>Backoff factors</h2>
<p>Calculation of backoff factors have lead to values greater than one for final Katz backoff "probabilities". A normalisation could be added.</p>
<h2>END and UNK</h2>
<p>Prababilities for these special tokens seems to be biased (e.g. END shows up too frequently as prediction, and therefore has been deleted from possible predictions, and UNK has been dropped from prediction in interactive mode, but not in accuracy mode). Further investigations could be performed to fix this.</p>
<h2>Combining predictors</h2>
<p>One can think of combining 2 or more among the 4 predictors, using a linear combination of their probabilities (e.g. Stupid backoff with stemming and Katz backoff).</p>
<h2>Interpolation methods</h2>
<p>Another class of algorithms could be tested, which interpolate probabilities of the different N-grams levels, as Kneser-Ney method, or Jelinek & Mercer.</p>
<h2>Secondary Stemming</h2>
<p>We can think of using stemmed grams only if non stemmed grams have given no results at a specific ngrams level, before backing off to lower level.</p>
<h2>Input history</h2>
<p>We coulsd use words typed by the user  to train the system following his habits. The idea is to interpolate between predictions and typing history.</p>
<h2>Processing / Memory optimisation</h2>
<p>To get lighter data objects and faster computations, one could think of storing probabilities in 2 small columns (8 bits / 4 bits for value and exponent), instead of a double precision numeric column.</p>
<h2>Use dictionaries</h2>
<p>Accuracy could be better with a dictionary, giving grammatical / type categories for each word (VERB, ADJECTIVE, ACTION.VERB, etc.), stored in parallel with ngrams values, like stemming was. This approach is available in the quanteda package, but implies further computations, on runtime as well.</p>
<h2>Other special words</h2>
<p>Like "unknown" words were translated to UNK token durin training, numbers could have been translated to NUMBER token, and so on for other special word types (hashtags, URLs, ...).</p>
<h2>Other methods</h2>
<p>Many other techniques could have been used for prediction, including neural networks, tree classifiers, etc.</p>
<p>Swiftkey has changed his method recently by the way, switching to neural network.</p>


</body>
</html>
